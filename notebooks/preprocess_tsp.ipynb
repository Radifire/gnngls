{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-pollution",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "backend = 'pytorch'\n",
    "os.environ['DGLBACKEND'] = backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-visit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# import tensorflow as tf\n",
    "import dgl\n",
    "import networkx as nx\n",
    "import tqdm.auto as tqdm\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pickle\n",
    "import pathlib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-america",
   "metadata": {},
   "source": [
    "# Compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-petersburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_tour(g, depot, weight='weight'):\n",
    "    tour = [depot]\n",
    "    while len(tour) < len(g.nodes):\n",
    "        i = tour[-1]\n",
    "        neighbours = [(j, g.edges[(i, j)]['weight']) for j in g.neighbors(i) if j not in tour]\n",
    "        j, dist = min(neighbours, key=lambda e: e[1])\n",
    "        tour.append(j)\n",
    "\n",
    "    tour.append(depot)\n",
    "    return tour\n",
    "\n",
    "def set_greedy_tour(g, depot):\n",
    "    tour = greedy_tour(g, depot)\n",
    "    tour_edges = zip(tour[:-1], tour[1:])\n",
    "\n",
    "    nx.set_edge_attributes(g, False, 'in_greedy_solution')\n",
    "    for e in tour_edges:\n",
    "        g.edges[e]['in_greedy_solution'] = True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-corner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_neighbour_features(g, nn_levels, min_degree):\n",
    "    # calculate knn for each edge\n",
    "    for e in g.edges:\n",
    "        g.edges[e]['neighbour'] = {}\n",
    "\n",
    "    for i in g.nodes:\n",
    "        neighbours = [(j, g.edges[(i, j)]['weight']) for j in g.neighbors(i)]\n",
    "        nearest_neighbours = sorted(neighbours, key=lambda e: e[1])\n",
    "        for k, (j, _) in enumerate(nearest_neighbours):\n",
    "            g.edges[(i, j)]['neighbour'][i] = k\n",
    "            \n",
    "    # knn graphs and nn clique\n",
    "    nx.set_edge_attributes(g, False, 'nn_clique')\n",
    "    for i, level in enumerate(nn_levels):\n",
    "        nx.set_edge_attributes(g, False, f'{i}_nn')\n",
    "\n",
    "    for e in g.edges:\n",
    "        i, j = e\n",
    "        neighbours = g.edges[e]['neighbour']\n",
    "        if neighbours[i] == neighbours[j]:\n",
    "            g.edges[e]['nn_clique'] = True\n",
    "\n",
    "        for level_i, level in enumerate(nn_levels):\n",
    "            g.edges[(i, j)][f'{level_i}_nn'] = (neighbours[i] <= level) or (neighbours[j] <= level)\n",
    "            \n",
    "    # erode longest edges until min degree reached\n",
    "    edges = sorted([(e, g.edges[e]['weight']) for e in g.edges], key=lambda e: e[1], reverse=True)\n",
    "    edges, _ = map(list, zip(*edges))\n",
    "\n",
    "    h = g.edge_subgraph(edges)\n",
    "    while min(dict(nx.degree(h)).values()) > min_degree:\n",
    "        edges.pop(0)\n",
    "        h = g.edge_subgraph(edges)\n",
    "\n",
    "    nx.set_edge_attributes(g, False, 'md_nn')\n",
    "    for e in edges:\n",
    "        g.edges[e]['md_nn'] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-electronics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_depot_weight(g, depot):\n",
    "    for n in g.nodes:\n",
    "        if n == depot:\n",
    "            g.nodes[n]['depot_weight'] = 0\n",
    "        else:\n",
    "            g.nodes[n]['depot_weight'] = g.edges[(depot, n)]['weight']\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_fancy_graph_features(g):\n",
    "    cf_bc = nx.edge_current_flow_betweenness_centrality(g, weight='weight')\n",
    "    sp_bc = nx.edge_betweenness_centrality(g, weight='weight')\n",
    "\n",
    "    sp_cc = nx.closeness_centrality(g, distance='weight')\n",
    "    cf_cc = nx.current_flow_closeness_centrality(g, weight='weight')\n",
    "    cl = nx.clustering(g, weight='weight')\n",
    "\n",
    "    nx.set_edge_attributes(g, sp_bc, 'sp_betweenness')\n",
    "    nx.set_edge_attributes(g, cf_bc, 'cf_betweenness')\n",
    "    nx.set_node_attributes(g, sp_cc, 'sp_closeness')\n",
    "    nx.set_node_attributes(g, cf_cc, 'cf_closeness')\n",
    "    nx.set_node_attributes(g, cl, 'clustering')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-fashion",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-happening",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('/local/scratch/bh511/data')\n",
    "data_file = data_dir / 'tsp10_concorde_1000000.txt'\n",
    "\n",
    "n_instances = 150000\n",
    "out_dir = data_dir / f'{n_instances}_instances'\n",
    "out_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-prairie",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_line(line):\n",
    "    problem_str, solution_str = line.split('output')\n",
    "\n",
    "    node_data = iter(problem_str.strip().split(' '))\n",
    "    node_counter = 0\n",
    "    nodes = {}\n",
    "    for x in node_data:\n",
    "        y = next(node_data)\n",
    "        pos = np.array([float(x), float(y)])\n",
    "        nodes[node_counter] = pos\n",
    "        node_counter += 1\n",
    "\n",
    "    solution = [int(x) - 1 for x in solution_str.strip().split(' ')]\n",
    "    solution_edges = [e for e in zip(solution[:-1], solution[1:])]\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    for i in nodes:\n",
    "        G.add_node(i, pos=nodes[i])\n",
    "    for i, j in itertools.combinations(nodes, 2):\n",
    "        w = np.linalg.norm(nodes[i] - nodes[j])\n",
    "        in_solution = (i, j) in solution_edges or (j, i) in solution_edges\n",
    "        G.add_edge(i, j, weight=w, in_solution=in_solution)\n",
    "        \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_file) as data:\n",
    "    for instance_i in tqdm.trange(n_instances): \n",
    "        g = parse_line(next(data))\n",
    "        \n",
    "        depot = next(iter(g.nodes))\n",
    "        set_greedy_tour(g, depot)\n",
    "        set_depot_weight(g, depot)\n",
    "\n",
    "        nn_levels = [int(x*len(g.nodes)) for x in [0.1, 0.2, 0.3]]\n",
    "        min_degree = 2\n",
    "        set_neighbour_features(g, nn_levels, min_degree)\n",
    "\n",
    "        set_fancy_graph_features(g)\n",
    "\n",
    "        # create new graph with just features/labels (and pos for plotting)\n",
    "        h = g.__class__()\n",
    "        h.add_nodes_from(g)\n",
    "        h.add_edges_from(g.edges)\n",
    "        nx.set_node_attributes(h, nx.get_node_attributes(g, 'pos'), 'pos')\n",
    "\n",
    "        for e in h.edges:\n",
    "            i, j = e\n",
    "            features = np.array([\n",
    "                g.edges[e]['weight'],\n",
    "                g.edges[e]['in_greedy_solution'],\n",
    "                g.edges[e]['neighbour'][i],\n",
    "                g.edges[e]['neighbour'][j],\n",
    "                g.edges[e]['nn_clique'],\n",
    "                g.edges[e]['0_nn'],\n",
    "                g.edges[e]['1_nn'],\n",
    "                g.edges[e]['2_nn'],\n",
    "                g.edges[e]['md_nn'],\n",
    "                g.edges[e]['sp_betweenness'], # the same for every node\n",
    "                g.edges[e]['cf_betweenness'],\n",
    "                g.nodes[i]['depot_weight'],\n",
    "                g.nodes[j]['depot_weight'],\n",
    "                g.nodes[i]['sp_closeness'],\n",
    "                g.nodes[j]['sp_closeness'],\n",
    "                g.nodes[i]['cf_closeness'],\n",
    "                g.nodes[j]['cf_closeness'],\n",
    "                g.nodes[i]['clustering'],\n",
    "                g.nodes[j]['clustering'],\n",
    "            ], dtype=np.float32)\n",
    "            label = np.array([\n",
    "                g.edges[e]['in_solution'],\n",
    "            ], dtype=np.int64)\n",
    "\n",
    "            h.edges[e]['x'] = features\n",
    "            h.edges[e]['y'] = label\n",
    "            \n",
    "        nx.write_gpickle(h, out_dir / f'i{instance_i}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-brave",
   "metadata": {},
   "source": [
    "# Split into train, validation, test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-lending",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = list(out_dir.glob('i[0-9]*.pkl'))\n",
    "\n",
    "train_set, test_set = train_test_split(data_set, train_size=0.8, shuffle=True)\n",
    "train_set, val_set = train_test_split(train_set, train_size=0.8, shuffle=True)\n",
    "\n",
    "for data_set, file in zip([train_set, val_set, test_set], ['train.txt', 'val.txt', 'test.txt']):\n",
    "    with open(out_dir / file, 'w') as data_file:\n",
    "        for path in data_set:\n",
    "            data_file.write(str(path) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-hours",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "for instance_path in tqdm.tqdm(train_set):\n",
    "    g = nx.read_gpickle(instance_path)\n",
    "    features = np.vstack(list(g.edges[e]['x'] for e in g.edges))\n",
    "    scaler.partial_fit(features)\n",
    "    \n",
    "pickle.dump(scaler, open(out_dir / 'scaler.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-postcard",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_graphs(instances, scaler):\n",
    "    graphs = []\n",
    "    for instance in tqdm.tqdm(instances):\n",
    "        g = nx.read_gpickle(instance)\n",
    "        lg = nx.line_graph(g)\n",
    "        \n",
    "        features = {e: scaler.transform(g.edges[e]['x'][np.newaxis, :]).squeeze() for e in lg.nodes}\n",
    "        labels = {e: g.edges[e]['y'] for e in lg.nodes}\n",
    "        nx.set_node_attributes(lg, features, 'x')\n",
    "        nx.set_node_attributes(lg, labels, 'y')\n",
    "\n",
    "        h = dgl.from_networkx(lg, node_attrs=['x', 'y'])\n",
    "        graphs.append(h)\n",
    "    \n",
    "    return graphs\n",
    "\n",
    "train_graphs = prepare_graphs(train_set, scaler)\n",
    "dgl.save_graphs(str(out_dir / 'train_graphs.bin'), train_graphs)\n",
    "\n",
    "val_graphs = prepare_graphs(val_set, scaler)\n",
    "dgl.save_graphs(str(out_dir / 'val_graphs.bin'), val_graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-tackle",
   "metadata": {},
   "source": [
    "# Visualise dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-category",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmap_colors = np.zeros((100, 4))\n",
    "# cmap_colors[:, 0] = 1\n",
    "# cmap_colors[:, 3] = np.linspace(0, 1, 100)\n",
    "# cmap = ListedColormap(cmap_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-charity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# for i, (ax, g) in enumerate(zip(axes, nx_graphs[:3])):\n",
    "#     pos = {n: (g.nodes[n]['x'], g.nodes[n]['y']) for n in g.nodes}\n",
    "#     in_solution = nx.get_edge_attributes(g, 'in_solution')\n",
    "\n",
    "#     nx.draw(g, pos, edge_color=in_solution.values(), ax=ax, edge_cmap=cmap, edge_vmax=1, edge_vmin=0)\n",
    "#     ax.set_title(f'Instance {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-enlargement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "# g = nx_graphs[0]\n",
    "\n",
    "# feature_name = 'in_solution'\n",
    "# pos = {n: (g.nodes[n]['x'], g.nodes[n]['y']) for n in g.nodes}\n",
    "# feature = nx.get_edge_attributes(g, feature_name)\n",
    "\n",
    "# nx.draw(g, pos, edge_color=feature.values(), ax=ax, edge_cmap=cmap)\n",
    "# ax.set_title(feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-profession",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
